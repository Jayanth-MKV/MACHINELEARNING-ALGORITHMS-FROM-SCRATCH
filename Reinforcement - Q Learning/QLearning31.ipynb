{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM1ICNoQe2DNt/gv0MVEmOX"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":68,"metadata":{"id":"hZPArv269Qse","executionInfo":{"status":"ok","timestamp":1679291729541,"user_tz":-330,"elapsed":3,"user":{"displayName":"Jayanth MKV","userId":"04368715744450558786"}}},"outputs":[],"source":["import numpy as np\n","\n","class Environment:\n","    def __init__(self):\n","        self.num_states = 6\n","        self.num_actions = 4\n","        self.reward_table = np.array([\n","            [1, 1, 100],\n","            [1, 1, 1,],\n","        ])\n","        self.location=np.array([\n","            [1,2,6],\n","            [3,4,5]\n","        ])\n","        self.actions=np.array([\"up\",\"right\",\"down\",\"left\"])\n","\n","    def reset(self):\n","        self.state = np.random.randint(1, self.num_states+1)\n","        return self.state\n","\n","    def step(self, action):\n","        r,c=np.argwhere(self.location==self.state)[0]\n","        next_state = self.get_next_location(r,c,action)\n","        reward = self.reward_table[r, c]\n","        done = (next_state == 6)\n","        self.state = next_state\n","        return next_state, reward, done\n","    \n","    def _step(self,state,action):\n","        r,c=np.argwhere(self.location==state)[0]\n","        next_state = self.get_next_location(r,c,action)\n","        return next_state\n","\n","    def get_next_location(self,current_row_index, current_column_index, action_index):\n","        new_row_index = current_row_index\n","        new_column_index = current_column_index\n","        if self.actions[action_index] == 'up' and current_row_index > 0:\n","          new_row_index -= 1\n","        elif self.actions[action_index] == 'right' and current_column_index < 2:\n","          new_column_index += 1\n","        elif self.actions[action_index] == 'down' and current_row_index < 1:\n","          new_row_index += 1\n","        elif self.actions[action_index] == 'left' and current_column_index > 0:\n","          new_column_index -= 1\n","        return self.location[new_row_index, new_column_index]\n","\n","class QLearning:\n","    def __init__(self, env, num_episodes, gamma=0.8, alpha=0.5, epsilon=0.1):\n","        self.env = env\n","        self.num_episodes = num_episodes\n","        self.gamma = gamma\n","        self.prev = None\n","        self.epsilon = epsilon\n","        self.q_table = np.zeros((env.num_states, env.num_actions))\n","\n","    def choose_action(self, state,epsilon=None):\n","        if epsilon==None:\n","          epsilon=self.epsilon\n","        if np.random.random() < epsilon:\n","          # print(\"SP\")\n","          return np.argmax(self.q_table[state-1, :])\n","        else:\n","          return np.random.randint(0, self.env.num_actions)\n","\n","    def train(self):\n","        for episode in range(self.num_episodes):\n","            state = self.env.reset()\n","            done = False\n","            \n","            while not done:\n","                action = self.choose_action(state)\n","                # print(state,\" : \",action)\n","                next_state, reward, done = self.env.step(action)\n","                # r,c=np.argwhere(self.env.location==state)[0]\n","                self.q_table[state-1, action] = reward + self.gamma * np.max(self.q_table[next_state-1, :])\n","                state = next_state\n","        return self.q_table\n","\n","    def get_shortest_path(self,state):\n","        #return immediately if this is an invalid starting location\n","        if state==6:\n","          return []\n","        else: #if this is a 'legal' starting location\n","          shortest_path = []\n","          shortest_path.append([state])\n","          cnt=0\n","          #continue moving along the path until we reach the goal (i.e., the item packaging location)\n","          while not state==6:\n","            cnt+=1\n","            #get the best action to take\n","            action = self.choose_action(state, 1.)\n","            print(state,\" : \",action)\n","            #move to the next location on the path, and add the new location to the list\n","            state= self.env._step(state,action)\n","            # print(\"NS\",next_state)\n","            shortest_path.append([state])\n","          return shortest_path\n"]},{"cell_type":"code","source":[],"metadata":{"id":"fSAU3upSgMBk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["env = Environment()\n","q_learning = QLearning(env, num_episodes=100)\n","q_table = q_learning.train()\n","print(q_table)\n","q_learning.get_shortest_path(5)"],"metadata":{"id":"e_lKLp-qfvyK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1679291795442,"user_tz":-330,"elapsed":560,"user":{"displayName":"Jayanth MKV","userId":"04368715744450558786"}},"outputId":"0aa8c6a8-5b7b-441c-bbdf-b37013f4ea30"},"execution_count":72,"outputs":[{"output_type":"stream","name":"stdout","text":["[[220.32216525 274.15270656 160.10470656 220.32216525]\n"," [228.73888    353.35270656 198.8808832  220.32216525]\n"," [198.8808832  198.8808832  160.10470656 160.10470656]\n"," [247.351104   274.15270656 198.8808832  160.10470656]\n"," [353.35270656 247.351104   247.351104   198.8808832 ]\n"," [440.4408832  425.551104   327.73888    346.351104  ]]\n","5  :  0\n"]},{"output_type":"execute_result","data":{"text/plain":["[[5], [6]]"]},"metadata":{},"execution_count":72}]},{"cell_type":"code","source":[],"metadata":{"id":"hDQTY7k_l0JS"},"execution_count":null,"outputs":[]}]}